{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 102672,
          "databundleVersionId": 12375409,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook54f14bf370",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "rNZkhsj_T3_h"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "soil_classification_path = kagglehub.competition_download('soil-classification')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "uEiuCOZTT3_i"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T06:58:41.248195Z",
          "iopub.execute_input": "2025-05-25T06:58:41.248475Z",
          "iopub.status.idle": "2025-05-25T06:58:41.253372Z",
          "shell.execute_reply.started": "2025-05-25T06:58:41.248455Z",
          "shell.execute_reply": "2025-05-25T06:58:41.252411Z"
        },
        "id": "En1MjpEvT3_i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bwDpdfn2T3_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Image parameters\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Dataset directories\n",
        "BASE_DIR = '/kaggle/input/soil-classification/soil_classification-2025'\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
        "TEST_DIR = os.path.join(BASE_DIR, 'test')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T06:58:45.149457Z",
          "iopub.execute_input": "2025-05-25T06:58:45.149801Z",
          "iopub.status.idle": "2025-05-25T06:58:45.154461Z",
          "shell.execute_reply.started": "2025-05-25T06:58:45.149772Z",
          "shell.execute_reply": "2025-05-25T06:58:45.153617Z"
        },
        "id": "af__pPHvT3_j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Test directory - point to the folder containing your images\n",
        "train_dir = '/kaggle/input/soil-classification/soil_classification-2025/train'  # Update if needed\n",
        "\n",
        "# Get all image paths (jpg, jpeg, webp)\n",
        "image_paths = sorted(\n",
        "    glob(os.path.join(train_dir, \"*.jpg\")) +     ##find all images and sort them alphabeticaaly\n",
        "    glob(os.path.join(train_dir, \"*.jpeg\")) +      ##ex: img1.jpg,img3.jpeg,img2.wbep  (arrays)\n",
        "    glob(os.path.join(train_dir, \"*.webp\"))\n",
        ")\n",
        "\n",
        "print(f\"Found {len(image_paths)} test images\")\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    # Read the raw file\n",
        "    img = tf.io.read_file(path)\n",
        "\n",
        "    # Try to decode based on file extension\n",
        "    if path.lower().endswith(('.jpg', '.jpeg')):\n",
        "        img = tf.image.decode_jpeg(img, channels=3)    ##channels=3 ie 3 colors red,green,blue\n",
        "    elif path.lower().endswith('.webp'):\n",
        "        # For WebP, we might need to use a different approach\n",
        "        try:\n",
        "            # Try decode_image first (works in TF 2.10+)\n",
        "            img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "        except:\n",
        "            # Fallback for older TF versions - may require additional dependencies\n",
        "            import webp\n",
        "            img = tf.numpy_function(lambda x: webp.load_image(x, mode='RGB'), [path], tf.uint8)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image format: {path}\")\n",
        "\n",
        "    img = tf.image.resize(img, [224, 224])\n",
        "    img = img / 255.0  # Normalize to [0,1]  convert pixel from[0,255] to [0,1] for faster training\n",
        "    return img\n",
        "\n",
        "# Build train dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "train_ds = train_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE) #num_parallel_calls=tf.data.AUTOTUNE tells TensorFlow to automatically tune how many parallel calls to make for best performance.\n",
        "\n",
        "\n",
        "train_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE) #  Batches the data: each batch contains 32 images.\n",
        "\n",
        "                                                         #Prefetches data for faster performance: while the model is training on one batch, the next batch is prepared in the background.\n",
        "\n",
        "#                                                            AUTOTUNE again optimizes performance automatically.\n",
        "\n",
        "# Verify first few images\n",
        "for i, image in enumerate(train_ds.take(1)):\n",
        "    print(f\"Batch {i} shape: {image.shape}\")\n",
        "    print(f\"Pixel range: {tf.reduce_min(image):.2f} to {tf.reduce_max(image):.2f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T06:58:48.605824Z",
          "iopub.execute_input": "2025-05-25T06:58:48.606475Z",
          "iopub.status.idle": "2025-05-25T06:58:48.662904Z",
          "shell.execute_reply.started": "2025-05-25T06:58:48.606448Z",
          "shell.execute_reply": "2025-05-25T06:58:48.661418Z"
        },
        "id": "iyvCWjN1T3_k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Test directory - point to the folder containing your images\n",
        "test_dir = '/kaggle/input/soil-classification/soil_classification-2025/test'  # Update if needed\n",
        "\n",
        "# Get all image paths (jpg, jpeg, webp)\n",
        "image_paths = sorted(\n",
        "    glob(os.path.join(test_dir, \"*.jpg\")) +\n",
        "    glob(os.path.join(test_dir, \"*.jpeg\")) +\n",
        "    glob(os.path.join(test_dir, \"*.webp\"))\n",
        ")\n",
        "\n",
        "print(f\"Found {len(image_paths)} test images\")\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    # Read the raw file\n",
        "    img = tf.io.read_file(path)\n",
        "\n",
        "    # Try to decode based on file extension\n",
        "    if path.lower().endswith(('.jpg', '.jpeg')):\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "    elif path.lower().endswith('.webp'):\n",
        "        # For WebP, we might need to use a different approach\n",
        "        try:\n",
        "            # Try decode_image first (works in TF 2.10+)\n",
        "            img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "        except:\n",
        "            # Fallback for older TF versions - may require additional dependencies\n",
        "            import webp\n",
        "            img = tf.numpy_function(lambda x: webp.load_image(x, mode='RGB'), [path], tf.uint8)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image format: {path}\")\n",
        "\n",
        "    img = tf.image.resize(img, [224, 224])\n",
        "    img = img / 255.0  # Normalize to [0,1]\n",
        "    return img\n",
        "\n",
        "# Build test dataset\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "test_ds = test_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Verify first few images\n",
        "for i, image in enumerate(test_ds.take(1)):\n",
        "    print(f\"Batch {i} shape: {image.shape}\")\n",
        "    print(f\"Pixel range: {tf.reduce_min(image):.2f} to {tf.reduce_max(image):.2f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T06:58:57.791523Z",
          "iopub.execute_input": "2025-05-25T06:58:57.792306Z",
          "iopub.status.idle": "2025-05-25T06:58:57.851783Z",
          "shell.execute_reply.started": "2025-05-25T06:58:57.792278Z",
          "shell.execute_reply": "2025-05-25T06:58:57.85019Z"
        },
        "id": "P8ykeGp4T3_l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = v=test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T06:59:04.504675Z",
          "iopub.execute_input": "2025-05-25T06:59:04.505035Z",
          "iopub.status.idle": "2025-05-25T06:59:04.526984Z",
          "shell.execute_reply.started": "2025-05-25T06:59:04.505011Z",
          "shell.execute_reply": "2025-05-25T06:59:04.525966Z"
        },
        "id": "sJSk7mV9T3_l"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'''import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Load labels\n",
        "df = pd.read_csv(\"/kaggle/input/soil-classification/soil_classification-2025/train_labels.csv\")\n",
        "\n",
        "# Label mapping\n",
        "label_to_index = {\n",
        "    'Alluvial soil': 0,\n",
        "    'Black Soil': 1,\n",
        "    'Clay soil': 2,\n",
        "    'Red soil': 3\n",
        "}\n",
        "df['label'] = df['soil_type'].map(label_to_index)  ## label\n",
        "                                                   #-----------\n",
        "                                                   ##alluvial,balck,red,alluvial--> 0,1,3,0\n",
        "\n",
        "\n",
        "# File paths and labels\n",
        "image_paths = [os.path.join(\"/kaggle/input/soil-classification/soil_classification-2025/train\", img_id) for img_id in df['image_id']]\n",
        "labels = tf.keras.utils.to_categorical(df['label'], num_classes=4).astype('float32')  #Converts numeric labels into one-hot encoded vectors.\n",
        "                                                                                       #Example: label 2 → [0, 0, 1, 0].\n",
        "# ✅ Explicit cast\n",
        "def decode_image_safe(filename, label):\n",
        "    def _load_image(filename_str, label_tensor):\n",
        "        try:\n",
        "            image = tf.io.read_file(filename_str.numpy().decode())\n",
        "            image = tf.image.decode_jpeg(image, channels=3)\n",
        "            image = tf.image.resize(image, [224, 224])\n",
        "            image = image / 255.0\n",
        "        except Exception as e:\n",
        "            image = tf.zeros([224, 224, 3])   ## if corrupt or non decoded return empty image with no label\n",
        "            label_tensor = tf.zeros([4])\n",
        "        return image, tf.cast(label_tensor, tf.float32)  # ✅ Cast to float32\n",
        "\n",
        "    image, label = tf.py_function(_load_image, [filename, label], [tf.float32, tf.float32])\n",
        "    image.set_shape([224, 224, 3])\n",
        "    label.set_shape([4])\n",
        "    return image, label\n",
        "\n",
        "\n",
        "\n",
        "# --- Dataset creation ---\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels)) \\\n",
        "    .shuffle(1000) \\\n",
        "    .map(decode_image_safe, num_parallel_calls=AUTOTUNE) \\\n",
        "    .batch(BATCH_SIZE) \\\n",
        "    .prefetch(AUTOTUNE)'''\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load labels\n",
        "df = pd.read_csv(\"/kaggle/input/soil-classification/soil_classification-2025/train_labels.csv\")\n",
        "\n",
        "# Label mapping\n",
        "label_to_index = {\n",
        "    'Alluvial soil': 0,\n",
        "    'Black Soil': 1,\n",
        "    'Clay soil': 2,\n",
        "    'Red soil': 3\n",
        "}\n",
        "df['label'] = df['soil_type'].map(label_to_index)\n",
        "\n",
        "# File paths and labels\n",
        "image_paths = [os.path.join(\"/kaggle/input/soil-classification/soil_classification-2025/train\", img_id) for img_id in df['image_id']]\n",
        "labels = df['label'].values  # Keep numeric for stratified split\n",
        "\n",
        "# Split into training and validation sets (70:30)\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    image_paths,\n",
        "    labels,\n",
        "    test_size=0.3,\n",
        "    stratify=labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Convert labels to one-hot\n",
        "train_labels_cat = tf.keras.utils.to_categorical(train_labels, num_classes=4).astype('float32')\n",
        "val_labels_cat = tf.keras.utils.to_categorical(val_labels, num_classes=4).astype('float32')\n",
        "\n",
        "# --- Image loading ---\n",
        "def decode_image_safe(filename, label):\n",
        "    def _load_image(filename_str, label_tensor):\n",
        "        try:\n",
        "            image = tf.io.read_file(filename_str.numpy().decode())\n",
        "            image = tf.image.decode_jpeg(image, channels=3)\n",
        "            image = tf.image.resize(image, [224, 224])\n",
        "            image = image / 255.0\n",
        "        except Exception:\n",
        "            image = tf.zeros([224, 224, 3])\n",
        "            label_tensor = tf.zeros([4])\n",
        "        return image, tf.cast(label_tensor, tf.float32)\n",
        "\n",
        "    image, label = tf.py_function(_load_image, [filename, label], [tf.float32, tf.float32])\n",
        "    image.set_shape([224, 224, 3])\n",
        "    label.set_shape([4])\n",
        "    return image, label\n",
        "\n",
        "# --- Dataset creation ---\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels_cat)) \\\n",
        "    .shuffle(1000) \\\n",
        "    .map(decode_image_safe, num_parallel_calls=AUTOTUNE) \\\n",
        "    .batch(BATCH_SIZE) \\\n",
        "    .prefetch(AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels_cat)) \\\n",
        "    .map(decode_image_safe, num_parallel_calls=AUTOTUNE) \\\n",
        "    .batch(BATCH_SIZE) \\\n",
        "    .prefetch(AUTOTUNE)\n",
        "\n",
        "print(f\"Train samples: {len(train_paths)}\")\n",
        "print(f\"Validation samples: {len(val_paths)}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T08:04:44.113977Z",
          "iopub.execute_input": "2025-05-25T08:04:44.114277Z",
          "iopub.status.idle": "2025-05-25T08:04:44.258539Z",
          "shell.execute_reply.started": "2025-05-25T08:04:44.114257Z",
          "shell.execute_reply": "2025-05-25T08:04:44.257546Z"
        },
        "id": "VtEEaqnUT3_m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# example simple model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=(224, 224, 3)),\n",
        "\n",
        "    # Block 1\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Block 2\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Block 3\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Dropout(0.4),\n",
        "\n",
        "    # Dense\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(4, activation='softmax')  # assuming 4 soil classes\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='Adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=4,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T09:23:16.727663Z",
          "iopub.execute_input": "2025-05-25T09:23:16.727976Z",
          "iopub.status.idle": "2025-05-25T09:41:42.706103Z",
          "shell.execute_reply.started": "2025-05-25T09:23:16.727929Z",
          "shell.execute_reply": "2025-05-25T09:41:42.705183Z"
        },
        "id": "Y4uPQVxST3_m"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "test_dir = '/kaggle/input/soil-classification/soil_classification-2025/test'\n",
        "test_df = pd.read_csv('/kaggle/input/soil-classification/soil_classification-2025/test_ids.csv')  # or however you're loading test image list\n",
        "\n",
        "# ✅ FIX FILE PATHS HERE\n",
        "test_df['image_path'] = test_df['image_id'].apply(lambda x: os.path.join(test_dir, x))\n",
        "\n",
        "# Now create your test dataset using test_df['image_path']\n",
        "def load_and_preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = image / 255.0\n",
        "    return image\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(test_df['image_path'].values)\n",
        "test_ds = test_ds.map(lambda x: load_and_preprocess_image(x)).batch(32)\n",
        "\n",
        "def safe_load_and_preprocess_image(path):\n",
        "    def _load_image(path_str):\n",
        "        try:\n",
        "            image = tf.io.read_file(path_str)\n",
        "            image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
        "            image = tf.image.resize(image, [224, 224])\n",
        "            image = image / 255.0\n",
        "            return image\n",
        "        except:\n",
        "            # If error occurs, return a blank image\n",
        "            return tf.zeros([224, 224, 3], dtype=tf.float32)\n",
        "\n",
        "    return tf.py_function(_load_image, [path], tf.float32)\n",
        "\n",
        "# Assume test_df contains a column \"image_path\" with full paths\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(test_df['image_path'].values)\n",
        "test_ds = test_ds.map(lambda x: safe_load_and_preprocess_image(x))\n",
        "test_ds = test_ds.batch(32)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T09:41:47.104854Z",
          "iopub.execute_input": "2025-05-25T09:41:47.105437Z",
          "iopub.status.idle": "2025-05-25T09:41:47.249418Z",
          "shell.execute_reply.started": "2025-05-25T09:41:47.105412Z",
          "shell.execute_reply": "2025-05-25T09:41:47.248569Z"
        },
        "id": "hsdbj_uGT3_n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities\n",
        "pred_probs = model.predict(test_ds, verbose=1)\n",
        "\n",
        "# Get predicted class indices\n",
        "pred_labels = tf.argmax(pred_probs, axis=1).numpy()\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T09:41:51.673423Z",
          "iopub.execute_input": "2025-05-25T09:41:51.67373Z",
          "iopub.status.idle": "2025-05-25T09:42:12.224812Z",
          "shell.execute_reply.started": "2025-05-25T09:41:51.673707Z",
          "shell.execute_reply": "2025-05-25T09:42:12.223781Z"
        },
        "id": "70XjLDabT3_n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Soil label mapping\n",
        "label_map = {\n",
        "    0: 'Alluvial soil',\n",
        "    1: 'Black Soil',\n",
        "    2: 'Clay soil',\n",
        "    3: 'Red soil'\n",
        "}\n",
        "\n",
        "# Step 1: Extract filename from path\n",
        "test_df['filename'] = test_df['image_path'].apply(lambda x: os.path.basename(x))\n",
        "\n",
        "# Step 2: Map predicted labels to soil names\n",
        "pred_soil_names = [label_map[label] for label in pred_labels]\n",
        "\n",
        "# Step 3: Create result DataFrame with readable soil types\n",
        "results_df = pd.DataFrame({\n",
        "    'image_id': test_df['filename'],\n",
        "    'soil_type': pred_soil_names\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "output_file = '/kaggle/working/submission.csv'\n",
        "\n",
        "results_df.to_csv(output_file, index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T09:43:24.227811Z",
          "iopub.execute_input": "2025-05-25T09:43:24.228598Z",
          "iopub.status.idle": "2025-05-25T09:43:24.239094Z",
          "shell.execute_reply.started": "2025-05-25T09:43:24.228564Z",
          "shell.execute_reply": "2025-05-25T09:43:24.238012Z"
        },
        "id": "LmlRvb9CT3_n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('/kaggle/working/submission.csv')\n",
        "df.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T09:43:38.155093Z",
          "iopub.execute_input": "2025-05-25T09:43:38.155408Z",
          "iopub.status.idle": "2025-05-25T09:43:38.16777Z",
          "shell.execute_reply.started": "2025-05-25T09:43:38.155384Z",
          "shell.execute_reply": "2025-05-25T09:43:38.166819Z"
        },
        "id": "g-wjdvPOT3_n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get model predictions (assuming pred_labels is already computed)\n",
        "\n",
        "\n",
        "# Optional: Check prediction distribution\n",
        "import pandas as pd\n",
        "print(\"Prediction distribution:\")\n",
        "print(pd.Series(pred_labels).value_counts())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T09:43:45.738736Z",
          "iopub.execute_input": "2025-05-25T09:43:45.739899Z",
          "iopub.status.idle": "2025-05-25T09:43:45.749244Z",
          "shell.execute_reply.started": "2025-05-25T09:43:45.739856Z",
          "shell.execute_reply": "2025-05-25T09:43:45.747322Z"
        },
        "id": "oWZLwsSGT3_n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Try this alternative way to extract labels if the structure is different\n",
        "y_true = np.array([])\n",
        "for batch in test_ds:\n",
        "    if isinstance(batch, tuple):  # If batch contains (images, labels)\n",
        "        _, labels = batch\n",
        "        y_true = np.concatenate([y_true, labels.numpy()])\n",
        "    else:  # If batch contains only images\n",
        "        print(\"Warning: No labels found in test_ds\")\n",
        "        break\n",
        "\n",
        "if len(y_true) > 0:\n",
        "    # Convert if one-hot encoded\n",
        "    if len(y_true.shape) > 1 and y_true.shape[1] > 1:\n",
        "        y_true_labels = np.argmax(y_true, axis=1)\n",
        "    else:\n",
        "        y_true_labels = y_true\n",
        "\n",
        "    # Compute F1 score\n",
        "    from sklearn.metrics import f1_score\n",
        "    f1 = f1_score(y_true_labels, pred_labels, average='weighted')\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "else:\n",
        "    print(\"Cannot compute F1 - no labels found in test_ds\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T09:43:50.902846Z",
          "iopub.execute_input": "2025-05-25T09:43:50.903175Z",
          "iopub.status.idle": "2025-05-25T09:43:51.175163Z",
          "shell.execute_reply.started": "2025-05-25T09:43:50.903152Z",
          "shell.execute_reply": "2025-05-25T09:43:51.174129Z"
        },
        "id": "aHRdaM07T3_n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first batch\n",
        "sample_batch = next(iter(test_ds))\n",
        "print(\"Batch structure:\", sample_batch)\n",
        "\n",
        "# Check if it's a tuple (images, labels) or just images\n",
        "if isinstance(sample_batch, tuple):\n",
        "    print(\"Dataset contains labels (but might be empty)\")\n",
        "else:\n",
        "    print(\"Dataset contains only images - no labels available\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T09:43:54.158264Z",
          "iopub.execute_input": "2025-05-25T09:43:54.158597Z",
          "iopub.status.idle": "2025-05-25T09:43:54.427787Z",
          "shell.execute_reply.started": "2025-05-25T09:43:54.158576Z",
          "shell.execute_reply": "2025-05-25T09:43:54.4269Z"
        },
        "id": "10p4B7yoT3_o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check class distribution in predictions\n",
        "print(\"Predicted class distribution:\")\n",
        "print(pd.Series(pred_labels).value_counts())\n",
        "\n",
        "# Visualize (example for 5 classes)\n",
        "import matplotlib.pyplot as plt\n",
        "pd.Series(pred_labels).value_counts().plot(kind='bar')\n",
        "plt.title(\"Predicted Class Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T09:44:41.676803Z",
          "iopub.execute_input": "2025-05-25T09:44:41.677831Z",
          "iopub.status.idle": "2025-05-25T09:44:41.829971Z",
          "shell.execute_reply.started": "2025-05-25T09:44:41.6778Z",
          "shell.execute_reply": "2025-05-25T09:44:41.828906Z"
        },
        "id": "NJOxh8H5T3_o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    if 'val_accuracy' in history.history:\n",
        "        plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.title('AccuraCy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    if 'val_loss' in history.history:\n",
        "        plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
        "plt.legend()\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Call this after model.fit()\n",
        "plot_training_history(history)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T09:44:47.747994Z",
          "iopub.execute_input": "2025-05-25T09:44:47.748798Z",
          "iopub.status.idle": "2025-05-25T09:44:48.381381Z",
          "shell.execute_reply.started": "2025-05-25T09:44:47.748774Z",
          "shell.execute_reply": "2025-05-25T09:44:48.380368Z"
        },
        "id": "CvheFNXTT3_o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Get predictions and true labels from val_ds\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in val_ds:\n",
        "    preds = model.predict(images, verbose=0)\n",
        "    y_true.extend(tf.argmax(labels, axis=1).numpy())      # actual class indices\n",
        "    y_pred.extend(tf.argmax(preds, axis=1).numpy())       # predicted class indices\n",
        "\n",
        "# Step 2: Compute F1 Score\n",
        "f1 = f1_score(y_true, y_pred, average='macro')  # use 'weighted' or 'micro' as needed\n",
        "print(f\"F1 Score (macro): {f1:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-25T09:44:55.093625Z",
          "iopub.execute_input": "2025-05-25T09:44:55.093909Z",
          "iopub.status.idle": "2025-05-25T09:45:18.53984Z",
          "shell.execute_reply.started": "2025-05-25T09:44:55.09389Z",
          "shell.execute_reply": "2025-05-25T09:45:18.538452Z"
        },
        "id": "zKZb_QKeT3_o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 1: Define a mapping from numeric labels to soil type names\n",
        "# ---------------------------------------------\n",
        "# These labels are the human-readable equivalents of your model's numeric outputs.\n",
        "label_map = {\n",
        "    0: 'Alluvial Soil',\n",
        "    1: 'Black Soil',\n",
        "    2: 'Clay Soil',\n",
        "    3: 'Red Soil'\n",
        "}\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 2: Extract filenames from image paths\n",
        "# ---------------------------------------------\n",
        "# The image_path column contains full paths like \"/path/to/image.jpg\"\n",
        "# We only need the filename, like \"image.jpg\", for submission or display.\n",
        "test_df['filename'] = test_df['image_path'].apply(lambda x: os.path.basename(x))\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 3: Convert predicted numeric labels to soil names\n",
        "# ---------------------------------------------\n",
        "# Your model likely predicted values like 0, 1, 2, 3.\n",
        "# Use the mapping above to convert those numbers into meaningful names.\n",
        "pred_soil_names = [label_map[label] for label in pred_labels]\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 4: Create the final results DataFrame\n",
        "# ---------------------------------------------\n",
        "# This DataFrame contains the cleaned image filenames and their corresponding predicted soil types.\n",
        "results_df = pd.DataFrame({\n",
        "    'image_id': test_df['filename'],     # Just the filename, not full path\n",
        "    'soil_type': pred_soil_names         # Human-readable predicted soil type\n",
        "})\n",
        "\n",
        "# ---------------------------------------------\n",
        "# Step 5: Save the results to a CSV file\n",
        "# ---------------------------------------------\n",
        "# This file can be submitted or used for further analysis.\n",
        "output_path = '/kaggle/working/output/submission.csv'\n",
        "results_df.to_csv(output_path, index=False)\n",
        "\n",
        "# Confirmation\n",
        "print(f\"✅ Results saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "0cubvJhFT3_o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "EOMdKZr3T3_o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R_P8kotKT3_o"
      }
    }
  ]
}